"""
    Crawly - URL Crawler written in Python.

    Jordan Zdimirovic. https://github.com/jordstar20001/crawly
"""

# Import dependencies
from collections import deque
import requests, json, pandas as pd, numpy as np
from schema import Schema, And, Use, Optional, SchemaError
from crawlog import Crawlogger

OPTIONS_SCHEMA = Schema({
    # All source pages must have http / https, and must be strings. Must be at least one src
    "source_pages": And([str], lambda lst: len(lst) > 0 and all([map(v.startswith, ["http", "https"]) for v in lst])),

    # === OPTIONAL ===
    
    # Provide a logging file
    Optional("log_file", default=None): str,

    # Should logging show in the console?
    Optional("log_console", default=False): bool,

    # Max depth is default 0 (infinity), but must be 0 or positive
    Optional("max_depth", default=0): And(int, lambda x: x >= 0),

    # Asynchronous crawling?
    Optional("async", default=False): bool,

    # Export data to csv?
    Optional("csv_export", default=None): str,

    # Get geolocation data?
    Optional("geolocational", default=False): bool,

    # URL buffer size, determines how many URLs can be saved for later crawling.
    # If it is left default, the buffersize will be permitted to grow.
    Optional("url_buffersize", default=None): And(int, lambda x: x > 10),

    # Option to determine if depth traversal (i.e., LIFO) will be used, or
    # breadth traversal (i.e., FIFO).
    Optional("depth_first", default=False): bool
})

# Default Crawly exception
class CrawlyException(Exception): pass

def get_default_crawly_options():
    """
        Returns the default options that must be provided to the CrawlyCrawler constructor.
    """
    options = {
        "max_depth": 100,
        "source_pages": ["https://wikipedia.com"],
        "log_file": "crawler.log"
    }

    if not OPTIONS_SCHEMA.is_valid(options):
        raise CrawlyException(
            "Default options did not match the required schema.\nPlease check the source code or raise an issue on the GitHub."
        )

    return options


class CrawlyCrawler():
    def __init__(self, **kwa):
        """
            Crawly Crawler constructor.

            Keyword Arguments:
            * *options* (``dict``) --
            Provide options similar to that generated by `get_default_crawly_options()`.
            * *config* (``str``) --
            Provide a file path to a stored option JSON formatted file.
        """
        # Both options and config are not allowed
        assert not ("options" in kwa and "config" in kwa), "Cannot provide both an options dictionary and a config file"

        if "options" in kwa:
            self.options = kwa["options"]
        
        elif "config" in kwa:
            try:
                with open(kwa["config"]) as f:
                    self.options = json.loads(f.read())
            
            except FileNotFoundError:
                raise CrawlyException(f"Config file at '{kwa['config']}' was not found.")
            
            except json.decoder.JSONDecodeError:
                raise CrawlyException(f"Config file was invalid.")
        
        else:
            self.options = get_default_crawly_options()

        try:
            self.options = OPTIONS_SCHEMA.validate(self.options)

        except SchemaError as e:
            raise CrawlyException(f"Options were not valid.\n{e}")

        self.setup()

    def __get_next_job(self):
        # Depending on options, treat as queue or stack
        return self.url_jobs.popleft() if self.opt("depth_first") else self.url_jobs.pop()

    def __store_next_job(self, url: str):
        # Depending on options
        if len(self.url_jobs) == self.opt("url_buffersize"):
            raise BufferError(f"URL job buffer overflow: couldn't add URL: '{url}'.")

        self.url_jobs.append(url)

    def opt(self, name: str):
        """
            Returns the option with specified name
        """
        if name in self.options: return self.options[name]
        return None

    def setup(self):
        """
            Perform operations to setup the crawling process
        """
        # Setup logger
        self.logger = Crawlogger(
            show_in_console = self.opt("log_console"),
            fpath = self.opt("log_file"),
            insta_flush = True,
            name = "CRAWLY"
        )

        self.log = self.logger.log

        # Create a deque to store URLS to be processed
        self.url_jobs = deque(maxlen = self.opt("url_buffersize"))

        # Store each starting page into the URL jobs
        for page in self.opt("source_pages"):
            self.__store_next_job(page)
            

    def start(self):
        """
            Begin the crawling process
        """
        self.log("Crawling started")
